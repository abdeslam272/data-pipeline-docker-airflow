# data-pipeline-docker-airflow

.gitignore : Emp√™che d'ajouter des fichiers inutiles, tels que des fichiers temporaires ou des cl√©s API, au d√©p√¥t Git. Cela r√©duit le risque de partager des informations sensibles et garde votre d√©p√¥t propre.

.dockerignore : Lors de la construction de l'image Docker, cette liste exclut des fichiers qui ne sont pas n√©cessaires dans l'image, ce qui rend l'image plus l√©g√®re et plus rapide √† construire. Cela vous √©vite √©galement d'inclure des fichiers inutiles qui n‚Äôont pas de place dans l‚Äôimage, comme les fichiers de configuration locaux.

# Structure du projet
``` plaintext
data-pipeline-docker-airflow/
‚îú‚îÄ‚îÄ dags/
‚îÇ   ‚îú‚îÄ‚îÄ etl_pipeline.py       # DAG Airflow pour tester l'ETL en utilisant des scripts Python
‚îÇ   ‚îú‚îÄ‚îÄ dag.py                # DAG Airflow d√©velopp√© directement avec Airflow pour orchestrer l'ETL
‚îÇ
‚îú‚îÄ‚îÄ app/                      # Dossier contenant les scripts Python de l'ETL
‚îÇ   ‚îú‚îÄ‚îÄ api_handler.py        # Script Python pour extraire les donn√©es depuis une API
‚îÇ   ‚îú‚îÄ‚îÄ data_transformer.py   # Script Python pour transformer les donn√©es extraites
‚îÇ   ‚îú‚îÄ‚îÄ db_loader.py          # Script Python pour charger les donn√©es transform√©es dans PostgreSQL
‚îÇ
‚îú‚îÄ‚îÄ .dockerignore             # Fichier pour exclure des fichiers ou dossiers lors de la cr√©ation de l'image Docker
‚îú‚îÄ‚îÄ .gitignore                # Fichier pour exclure des fichiers ou dossiers dans le contr√¥le de version Git
‚îú‚îÄ‚îÄ docker-compose.yml        # Fichier pour orchestrer les services Docker (Airflow, PostgreSQL, etc.)
‚îú‚îÄ‚îÄ Dockerfile                # Fichier pour d√©finir l'image Docker principale pour vos scripts Python
‚îú‚îÄ‚îÄ requirements.txt          # Fichier listant les d√©pendances Python n√©cessaires au projet
‚îî‚îÄ‚îÄ README.md                 # Fichier de documentation du projet
```


# S√©curisation de la cl√© API
Pour √©viter d'exposer des informations sensibles, comme la cl√© API, ne la codez jamais directement dans le fichier source. Stockez-la plut√¥t dans une variable d'environnement ou un fichier .env, et chargez-la √† l'aide d'une biblioth√®que comme python-dotenv. Assurez-vous que le fichier .env est ajout√© √† .gitignore pour √©viter tout upload accidentel sur GitHub. Cette pratique prot√®ge vos identifiants et renforce la s√©curit√© de votre application.

# Que doit contenir un fichier docker-compose.yml ?
Un fichier docker-compose.yml d√©finit les services, r√©seaux et volumes de votre application. Chaque service sp√©cifie un conteneur, son image ou le contexte de build, les ports, les variables d'environnement et les d√©pendances. Les r√©seaux permettent la communication entre les services, et les volumes assurent la persistance des donn√©es. Il simplifie la gestion des configurations multi-conteneurs et le d√©ploiement des applications.
 
# Configure une t√¢che Airflow pour appeler api_handler.py p√©riodiquement.
This Airflow DAG, named simple_weather_pipeline, is scheduled to run every hour starting from December 14, 2024. It uses the BashOperator to execute a Python script (api_handler.py) located at /app/api_handler.py. The DAG is configured with retry logic (1 retry after a 5-minute delay) and catchup=False to avoid backfilling past runs. Default arguments specify that tasks do not depend on previous runs, and the workflow is suitable for automating API data collection or similar periodic tasks.

# Erreurs d'Airflow :
Apr√®s avoir lanc√© les conteneurs √† l'aide du fichier docker-compose situ√© dans le dossier airflow :
![image](https://github.com/user-attachments/assets/eef376f6-16ca-4868-90ca-2dd84569dff7)

J'observe une erreur r√©currente dans les deux conteneurs suivants : airflow-scheduler-1 et airflow-webserver-1.
L'erreur est la suivante : ![image](https://github.com/user-attachments/assets/975b4058-4860-4717-9941-4ede319739e1)

Solution: J'utilise pgAdmin pour int√©ragir avec la base de donn√©es, pas de naviguer en port 5432.


# Erreurs de conteneur postgres :
On a une erreur qui se r√©p√©te li√©e au container postgres : 
![image](https://github.com/user-attachments/assets/2a19fe93-9f43-461d-8ada-7ce73de5c5ff)


# le fonctionnement pour voir les donn√©es en pgadmin
### √âtape 1 : Ouvrir pgAdmin
1. Acc√©dez √† [http://localhost:5050](http://localhost:5050) dans votre navigateur.
2. Connectez-vous avec les identifiants suivants :
   - **Email** : `admin@example.com`
   - **Mot de passe** : `admin`

### √âtape 2 : Connectez-vous √† votre serveur PostgreSQL
1. Dans le panneau de gauche, faites un clic droit sur **Servers** et s√©lectionnez **Create ‚Üí Server...**.
2. Remplissez les informations suivantes :
   - **Onglet General** : Donnez un nom √† votre serveur, par exemple : `Postgres`.
   - **Onglet Connection** :
     - **Hostname/Address** : `airflow` (correspond au nom de service dans votre fichier `docker-compose.yml`).
     - **Port** : `5432`.
     - **Maintenance Database** : `airflow`.
     - **Username** : `airflow`.
     - **Password** : `airflow`.
3. Cliquez sur **Save**.

### √âtape 3 : Naviguez vers la base de donn√©es
1. Dans le panneau de gauche, d√©veloppez l‚Äôarborescence **Servers** :
   - Cliquez sur votre serveur (par exemple : `Postgres`).
   - D√©veloppez **Databases**.
   - S√©lectionnez votre base de donn√©es : `airflow`.

### √âtape 4 : Ouvrir l‚Äôoutil de requ√™tes
1. D√©veloppez **Schemas ‚Üí public ‚Üí Tables**.
2. Vous devriez voir votre table `weather_data` dans la liste.
3. Faites un clic droit sur `weather_data` et s√©lectionnez **Query Tool**.
   - Vous pouvez √©galement cliquer sur le menu **Tools** en haut, puis choisir **Query Tool**.

### √âtape 5 : Ex√©cuter une requ√™te pour voir les donn√©es
1. Dans l‚Äô√©diteur de requ√™tes, saisissez la commande SQL suivante :
   ```sql
   SELECT * FROM weather_forecast;
   ```
2. Cliquez sur l'ic√¥ne en forme d'√©clair (ou appuyez sur F5) pour ex√©cuter la requ√™te.
### √âtape 6 : V√©rifier les r√©sultats
Les r√©sultats de la requ√™te appara√Ætront dans le panneau de sortie en bas.
Vous devriez voir les donn√©es ins√©r√©es, par exemple
   ```plaintext
   time                  | temperature | humidity | weather
----------------------+-------------+----------+--------
2024-12-21 11:00:00  | 25.3        | 60       | Sunny
   ```

# Airflow
Apache Airflow is a platform to programmatically author, schedule, and monitor workflows.

## Features:
Define task dependencies.
Configure task retries in case of failures.
Integrate with other tools through Connections.

## Core Components
### Webserver
Fournit l'interface utilisateur d'Airflow.
Permet de surveiller et de r√©soudre les probl√®mes des pipelines de donn√©es.
### Scheduler
D√©termine quand les t√¢ches doivent √™tre ex√©cut√©es en fonction des d√©pendances et des calendriers.
### Metadata Database
Sert de m√©moire √† Airflow, stockant les √©tats des workflows et les m√©tadonn√©es.
Utilise fr√©quemment PostgreSQL ou MySQL.
### Executor
D√©finit comment les t√¢ches sont ex√©cut√©es (par exemple : localement, dans un cluster ou via Kubernetes).
### Worker
Ex√©cute les t√¢ches assign√©es par le Planificateur via l'Ex√©cuteur.
### Triggers
G√®re les t√¢ches qui attendent la r√©alisation d'√©v√©nements externes avant de continuer.

## Core Components
### DAG
Directed Acyclic graph
Aucun cycle
Une seule direction
### Operator
Les op√©rateurs encapsulent les t√¢ches et permettent de d√©finir les actions √† effectuer
Permet d'√©crire de nombreuses t√¢ches sans coder
#### Action Operator
Ex√©cutent une action sp√©cifique
#### Transfom Operator
Effectuent le transfert de donn√©es entre syst√®mes
#### sensor opertors 
Attendent une certaine condition
#### Trigger-Deferrable Operators 
Capteurs qui ne bloquent pas un worker
#### Custom Operatos
Cr√©√©s pour des besoins sp√©cifiques

## Airflow & Docker 
To deploy Airflow on Docker Compose, you should fetch docker-compose.yaml.
   ```
   curl -LfO 'https://airflow.apache.org/docs/apache-airflow/2.10.4/docker-compose.yaml'
   ```
the command is to start the container:
   ```
   docker-compose up airflow-init
   docker-compose up -d
   ```

https://devblogit.com/apache-airflow-tutorial-architecture-concepts-and-how-to-run-airflow-locally-with-docker

## Cr√©er notre premier DAG

### Param√®tres du DAG
- **dag_id** : Identifiant unique pour votre DAG. S'il est dupliqu√©, Airflow en attribuera un al√©atoire sans afficher d'erreur.  
- **description** : Une courte description de l'objectif de votre DAG pour une meilleure compr√©hension.  
- **start_date** : La date et l'heure √† partir desquelles le DAG commencera √† s'ex√©cuter.  
- **schedule_interval** : D√©termine la fr√©quence d'ex√©cution du DAG (par exemple, quotidien, horaire).  
- **catchup** :  
  - `True` : Airflow ex√©cutera toutes les ex√©cutions manqu√©es depuis la `start_date`.  
  - `False` : Airflow ignorera les ex√©cutions manqu√©es et ne lancera que l'instance la plus r√©cente.  

## Points importants √† conna√Ætre

1. **Fuseau horaire par d√©faut** :  
   Le fuseau horaire par d√©faut dans Airflow est **UTC**. Vous pouvez ajuster ce param√®tre dans le fichier `airflow.cfg` ou par DAG selon vos besoins.

2. **Comprendre l'ex√©cution des t√¢ches** :  
   Dans Airflow, l'ex√©cution des t√¢ches est planifi√©e en fonction de la **date de d√©but** (*start date*) et de l'**intervalle de planification** (*schedule interval*). Notez que l'ex√©cution r√©elle d'une t√¢che correspond √† la **date de d√©but plus l'intervalle de planification**.  
   Par exemple, si un DAG a une date de d√©but fix√©e au `2024-01-01` et un intervalle quotidien, la premi√®re ex√©cution aura lieu pour la date **2024-01-02**.

3. **Intervalle de planification : `timedelta` vs. expressions Cron (`* * * * *`)** :  
   - **`timedelta`** : Cet objet Python permet de d√©finir des intervalles de temps de mani√®re programmatique (par exemple, `schedule_interval=timedelta(days=1)` pour une planification quotidienne).  
   - **Expressions Cron** : Elles offrent un moyen plus flexible de d√©finir des plannings gr√¢ce √† une syntaxe sp√©cifique (par exemple, `schedule_interval="0 0 * * *"` pour une ex√©cution quotidienne √† minuit).  
   - Le choix entre ces deux m√©thodes d√©pend de la complexit√© de votre planification. Utilisez `timedelta` pour des intervalles simples et les expressions Cron pour des plannings plus complexes.

## Cr√©ation d'un DAG et d'une t√¢che

J'ai cr√©√© un DAG nomm√© `myfirstdag.py` avec une t√¢che pour cr√©er une table dans PostgreSQL :

```python
from airflow import DAG
from datetime import datetime, timedelta
from airflow.providers.postgres.operators.postgres import PostgresOperator

# D√©finir le DAG
with DAG(
    dag_id="myfirstdag",
    description="C'est mon premier DAG",
    start_date=datetime(2024, 12, 27, 10, 0),
    schedule_interval="*/5 * * * *",
    catchup=False,
    dagrun_timeout=timedelta(minutes=45),
    tags=["sales", "monthly"]
) as dag:
    create_table = PostgresOperator(
        task_id="create_table",
        postgres_conn_id="postgres_conn",
        sql='''
        CREATE TABLE IF NOT EXISTS customers(
        customer_id VARCHAR(50) NOT NULL,
        customer_name VARCHAR NOT NULL,
        address VARCHAR NOT NULL,
        birth_date DATE NOT NULL
        );
        '''
    )
```
Une fois que vous avez cr√©√© le fichier, ouvrez Command Prompt ou PowerShell, puis lancez le planificateur Airflow avec la commande suivante :
```Powershell
docker exec -it airflow-airflow-scheduler-1 /bin/bash
```

Ensuite, vous pouvez tester la t√¢che en ex√©cutant la commande suivante :

```Powershell
airflow tasks test myfirstdag create_table 2024-12-31
```
Cela ex√©cutera la t√¢che create_table pour le DAG myfirstdag √† la date sp√©cifi√©e (ici le 31 d√©cembre 2024).


## Probl√®me de Configuration 

### Description de l'erreur
Lors de l'acc√®s √† l'interface utilisateur d'Airflow, l'erreur suivante appara√Æt : 

> **Airflow Configuration**  
> Your Airflow administrator chose not to expose the configuration, most likely for security reasons.

### Capture d'√©cran de l'erreur
![Configuration Error](https://github.com/user-attachments/assets/510491f0-dc4e-4200-9801-eb43725bb6b8)

---

### Solution
Pour r√©soudre ce probl√®me et exposer la configuration dans l'interface web d'Airflow, vous devez d√©finir la variable d'environnement `AIRFLOW__WEBSERVER__EXPOSE_CONFIG` √† `True`. Voici les √©tapes pour y parvenir : 

1. Cr√©ez un fichier `.env` dans le r√©pertoire contenant votre fichier `docker-compose.yml` et ajoutez la variable suivante :
   ```env
   AIRFLOW__WEBSERVER__EXPOSE_CONFIG=True
   ```

2. Modifiez le fichier docker-compose.yml pour inclure ce fichier .env dans la configuration du service airflow-webserver :
 ```
 services:
  airflow-webserver:
    <<: *airflow-common
    command: webserver
    ports:
      - "8080:8080"
    env_file:
      - .env
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    restart: always
    depends_on:
      <<: *airflow-common-depends-on
      airflow-init:
        condition: service_completed_successfully
 ```
3. Red√©marrez vos conteneurs Docker pour appliquer les modifications :

```bash
docker-compose down
docker-compose up -d
 ```
Apr√®s ces √©tapes, la configuration d'Airflow devrait √™tre visible depuis l'interface web.

## Sensors dans Apache Airflow

Les **Sensors** dans Apache Airflow sont des op√©rateurs sp√©ciaux qui permettent de surveiller et d'attendre qu'un √©v√©nement particulier se produise. Une fois l'√©v√©nement d√©tect√©, le capteur d√©clenche l'ex√©cution de la t√¢che suivante dans le DAG.

### Types de Sensors
1. **File Sensor**  
   Ce capteur v√©rifie si un fichier sp√©cifique est disponible dans un dossier donn√©. Par exemple, lorsqu'un fichier attendu arrive dans le r√©pertoire cible, le capteur d√©clenche l'ex√©cution de la prochaine t√¢che.

2. **SQL Sensor**  
   Ce capteur surveille une base de donn√©es pour v√©rifier si un enregistrement particulier existe. Par exemple, dans un sc√©nario o√π vous chargez des donn√©es quotidiennement, le capteur peut v√©rifier si un enregistrement attendu est arriv√©. En cas d'absence de donn√©es, il peut √©mettre une alerte ou effectuer une autre action.

### Utilisation
Les Sensors sont particuli√®rement utiles dans les workflows d√©pendant d'√©v√©nements externes, comme l'arriv√©e de fichiers ou la disponibilit√© de donn√©es sp√©cifiques dans une base de donn√©es.



# Unification de projet 
Avec un contenu de fichier .env:
DB_HOST=postgres
DB_PORT=5432
DB_NAME=my_database
DB_USER=my_user
DB_PASSWORD=my_password
POSTGRES_USER=airflow
POSTGRES_PASSWORD=airflow
POSTGRES_DB=airflow

j'arrive pas a voir la table Weather_data
Avec 
.env :
DB_HOST=postgres
DB_PORT=5432
DB_NAME=my_database
DB_USER=my_user
DB_PASSWORD=my_password

je peux voir la data dans de la table Weather_data in pgadmin

## Commands
 ```bash
docker-compose -f docker-compose.yaml up --build
 ```
 ```bash
docker-compose -f docker-compose.yaml down
 ```

Ce lien pour avoir le docker compose de airflow:
https://airflow.apache.org/docs/apache-airflow/stable/howto/docker-compose/index.html


## Probl√®mes dans les scripts Python

### Erreur rencontr√©e
Lors de l'ex√©cution du script, j'ai rencontr√© l'erreur suivante :  
![Erreur Python](https://github.com/user-attachments/assets/f6472fa2-d58b-4c31-9223-0354f3aaae2a)

---

### Solution

1. **V√©rifier la commande utilis√©e dans le Dockerfile**  
   Assurez-vous que la commande de d√©marrage est correctement configur√©e pour ex√©cuter le script Python. La ligne suivante dans le fichier Dockerfile doit √™tre correcte :
   ```dockerfile
   CMD ["bash", "-c", "python api_handler.py"]
   ```
2. **Configurer le fichier docker-compose.yml pour charger les variables d'environnement**
Ajoutez l'option env_file dans votre fichier docker-compose.yml pour inclure les variables d'environnement depuis un fichier .env :
   ```yml
   services:
     python-app:
       build:
         context: .
       env_file:
         - .env
   ```

## Airflow Errors:
Nous avons rencontr√© un probl√®me o√π Airflow n‚Äôarrivait pas √† localiser le fichier api_handler.py pour l‚Äôex√©cuter comme un DAG.
![image](https://github.com/user-attachments/assets/8db6fbd3-0982-400f-bb7f-acb908a21e50)


## Une solution Possible:
Nous avons d√©cid√© de convertir la logique du fichier api_handler.py en un DAG natif d‚ÄôAirflow.

Bonne nouvelle : cette solution a fonctionn√© avec succ√®s !
![image](https://github.com/user-attachments/assets/b7cb1aed-42d8-46c8-802b-319857126d43)

---

## üìö Astuces et Le√ßons Apprises  

### ‚úÖ Airflow & DAGs  
- Un **DAG** (Directed Acyclic Graph) est un workflow compos√© de **t√¢ches interd√©pendantes**.  
- `PythonOperator` permet d'ex√©cuter directement du code Python.  
- **Utiliser `op_kwargs`** pour passer des arguments √† une fonction Python dans un DAG.  
- Toujours d√©finir **`start_date=datetime(202X, X, X)`** pour √©viter une ex√©cution imm√©diate de tous les jobs pass√©s.  
- **D√©sactiver `catchup=True`** si l‚Äôon ne veut pas rattraper les ex√©cutions pass√©es.  

### ‚úÖ Docker & Orchestration  
- **Docker Compose** est id√©al pour orchestrer Airflow et PostgreSQL ensemble.  
- **Utiliser `.dockerignore`** pour √©viter d'inclure des fichiers inutiles dans l‚Äôimage Docker.  
- **Toujours v√©rifier les logs avec `docker-compose logs -f airflow-scheduler`** en cas de probl√®me.  

### ‚úÖ PostgreSQL & Gestion des Donn√©es  
- **Utiliser `psycopg2`** pour interagir avec une base PostgreSQL en Python.  
- **G√©rer les erreurs avec `try-except`** pour √©viter des interruptions de pipeline.  
- **Cr√©er automatiquement les tables** pour √©viter les erreurs de chargement.  

### ‚úÖ S√©curit√© et Bonnes Pratiques  
- **Stocker les cl√©s API et identifiants de la base de donn√©es dans un fichier `.env`**.  
- **Ne jamais versionner `.env`**, le lister dans `.gitignore`.  
- **Utiliser des logs et des messages d'erreur explicites** pour faciliter le debugging.  

---

## üìå R√©sum√© des Technologies Utilis√©es  

| üîß Technologie | üìú Usage |
|--------------|--------|
| **Apache Airflow** | Orchestration et automatisation du pipeline de donn√©es |
| **Python** | D√©veloppement des scripts ETL |
| **PostgreSQL** | Stockage des donn√©es m√©t√©orologiques |
| **Docker & Docker Compose** | Conteneurisation et orchestration des services |
| **psycopg2** | Connexion √† PostgreSQL depuis Python |
| **requests** | Requ√™tes API pour r√©cup√©rer les donn√©es m√©t√©o |

